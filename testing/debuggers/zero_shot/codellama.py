# -*- coding: utf-8 -*-
"""codellama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pwz3IZBWTsfyMJCdlDuuSsdiS3eQIc8y
"""

"""
codellama.py: Zero-shot debugger using CodeLlama
"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

class CodeLlamaDebugger:
    def __init__(self):
        self.model_name = "codellama/CodeLlama-7b-Instruct-hf"

        # Initialize model
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")

        # Load tokenizer and model
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)

        print(f"Loading model: {self.model_name}")
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
        self.model.eval()

        print("CodeLlama model loaded successfully!")

    def debug_code(self, task: str, buggy_code: str, max_new_tokens: int = 512) -> str:
        """Zero-shot debugging (no error message)"""
        # Format prompt for zero-shot debugging
        full_prompt = self.format_debug_prompt(task, buggy_code)

        try:
            # Prepare input (CodeLlama has a different prompt format)
            inputs = self.tokenizer(full_prompt, return_tensors="pt")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # Generate response
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.input_ids,
                    max_new_tokens=max_new_tokens,
                    do_sample=False,
                    top_k=50,
                    top_p=0.95,
                    temperature=0.2
                )

            # Decode response
            fixed_code = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            return fixed_code.strip()
        except Exception as e:
            print(f"Error in generation: {e}")
            return f"Error in generation: {e}"

    def format_debug_prompt(self, task: str, buggy_code: str) -> str:
        """Format prompt for zero-shot debugging with CodeLlama"""
        return (
            f"<s>[INST] <<SYS>>\n"
            f"You are a helpful AI coding assistant.\n"
            f"<</SYS>>\n\n"
            f"I was trying to solve the following task:\n"
            f"{task}\n\n"
            f"The code I wrote is:\n{buggy_code}\n\n"
            f"Please provide the corrected version of the code. [/INST]\n"
        )