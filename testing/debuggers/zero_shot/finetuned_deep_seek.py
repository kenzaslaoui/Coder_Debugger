# -*- coding: utf-8 -*-
"""finetuned_deep_seek.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/197Z09GClMAYHt1KPR_BG2sGsPg0dsvVe
"""

"""
finetuned_deep_seek.py: Zero-shot debugger using fine-tuned DeepSeek Coder
"""
import torch
from peft import PeftModel, PeftConfig
from transformers import AutoModelForCausalLM, AutoTokenizer
import os
from google.colab import drive
drive.mount('/content/drive')


class FinetunedDeepSeekDebugger:
    def __init__(self):
        # Path to the fine-tuned model adapter
        self.adapter_path = "/content/drive/MyDrive/Academic_Professional_Planning/SPRING2025/GEN_AI/Multi_Agent_Self_Debugger_Coder/training/finetuned/deepseek_coder_finetuned/final_finetuned_model"
        self.base_model = "deepseek-ai/deepseek-coder-6.7b-instruct"

        # Initialize model
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Using device: {self.device}")

        # Check if adapter files exist
        print(f"Checking adapter path: {self.adapter_path}")
        if not os.path.exists(self.adapter_path):
            raise ValueError(f"Adapter path doesn't exist: {self.adapter_path}")

        # List files in adapter directory
        print("Files in adapter directory:")
        for file in os.listdir(self.adapter_path):
            print(f"- {file}")

        # Look for adapter_config.json specifically
        adapter_config_path = os.path.join(self.adapter_path, "adapter_config.json")
        if not os.path.exists(adapter_config_path):
            print(f"adapter_config.json not found. Looking for it in subdirectories...")
            # Look in subdirectories
            for subdir in [d for d in os.listdir(self.adapter_path) if os.path.isdir(os.path.join(self.adapter_path, d))]:
                subdir_path = os.path.join(self.adapter_path, subdir)
                print(f"Checking subdirectory: {subdir_path}")
                if os.path.exists(os.path.join(subdir_path, "adapter_config.json")):
                    print(f"Found adapter_config.json in {subdir_path}")
                    self.adapter_path = subdir_path
                    break

        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model, trust_remote_code=True)

        # Load base model with reduced precision
        print(f"Loading base model: {self.base_model}")
        base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )

        # Trying to load the adapter
        try:
            print(f"Loading fine-tuned adapter from: {self.adapter_path}")

            # Try loading with local adapter path
            self.model = PeftModel.from_pretrained(
                base_model,
                self.adapter_path,
                local_files_only=True  # Force using local files
            )
            print("Successfully loaded adapter!")
        except Exception as e:
            print(f"Error loading adapter: {e}")

            # Fall back to base model if adapter loading fails
            print("Falling back to base model")
            self.model = base_model

        self.model.eval()

        print("Fine-tuned model loaded successfully!")

    def debug_code(self, task: str, buggy_code: str, max_new_tokens: int = 1024) -> str:
        """
        Zero-shot debugging (no error message)

        Args:
            task: Description of the task
            buggy_code: Code with bugs to be fixed
            max_new_tokens: Maximum number of tokens to generate

        Returns:
            String containing the debugged code or explanation
        """
        # Format prompt for zero-shot debugging
        full_prompt = self.format_debug_prompt(task, buggy_code)
        messages = [{'role': 'user', 'content': full_prompt}]

        try:
            # Prepare input
            inputs = self.tokenizer.apply_chat_template(
                messages,
                add_generation_prompt=True,
                return_tensors="pt"
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # Generate response
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=False,
                    top_k=50,
                    top_p=0.95,
                    temperature=0.2,
                    eos_token_id=self.tokenizer.eos_token_id
                )

            # Decode response
            fixed_code = self.tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)
            return fixed_code.strip()
        except Exception as e:
            print(f"Error in generation: {e}")
            return f"Error in generation: {e}"

    def format_debug_prompt(self, task: str, buggy_code: str) -> str:
      return (
          f"Given a programming task and its incorrect solution, your task is to fix up the incorrect solution according to the programming task and provide the correct, executable solution.\n\n"
          f"###Task:\n\n"
          f"{task}\n\n"
          f"###Incorrect Solution:\n\n"
          f"{buggy_code}\n"
      )