# -*- coding: utf-8 -*-
"""Copy of finetune_lora.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xDyeEXcp2RiUCeLBuuP97ivnNDEdq5eQ
"""

import os
os.kill(os.getpid(), 9)

!pip uninstall bitsandbytes

!pip uninstall torch

import torch
torch.cuda.empty_cache()

!nvcc --version
!which nvcc

!sudo rm -rf /usr/local/cuda

!cat /etc/os-release

!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
!sudo dpkg -i cuda-keyring_1.1-1_all.deb
!sudo apt-get update

!sudo apt-get -y install cuda-toolkit-12-4

import os

os.environ["CUDA_HOME"] = "/usr/local/cuda-12.4"
os.environ["PATH"] = "/usr/local/cuda-12.4/bin:" + os.environ["PATH"]
os.environ["LD_LIBRARY_PATH"] = "/usr/local/cuda-12.4/lib64:" + os.environ.get("LD_LIBRARY_PATH", "")

!nvcc --version
!which nvcc
!ls /usr/local | grep cuda

!sudo ln -s /usr/local/cuda-12.4 /usr/local/cuda

!nvcc --version
!nvidia-smi

!apt-get install -y build-essential cmake

import os
os.environ["BNB_CUDA_VERSION"] = "124"
os.environ["LD_LIBRARY_PATH"] = "/usr/local/cuda-12.4/lib64:" + os.environ.get("LD_LIBRARY_PATH", "")

!nvcc --version #This will display the CUDA version that the nvcc compiler is using.
!nvidia-smi

print(torch.__version__)  # Should print 2.5.1
print(torch.cuda.is_available())  # Should return True if CUDA is available

!pip uninstall torch -y
!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124

import torch
print(torch.__version__)  # Should print 2.5.1
print(torch.cuda.is_available())  # Should return True if CUDA is available

# !pip uninstall bitsandbytes

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/bitsandbytes-foundation/bitsandbytes.git
# %cd bitsandbytes
!mkdir -p build
!cmake -DCOMPUTE_BACKEND=cuda -DBUILD_CUDA_VERSION=12.4 -S . -B build
!cmake --build build -j8
!pip install -e .

import bitsandbytes as bnb
print(bnb.__cuda_version__)

import bitsandbytes as bnb
print(hasattr(bnb, "nn"))  # should be True
print(hasattr(bnb.nn, "Linear4bit"))  # should be True

!nvcc --version

import bitsandbytes as bnb

import torch
print(torch.__version__)  # Should print 2.5.1
print(torch.cuda.is_available())  # Should return True if CUDA is available

# Check the environment variable is set correctly
!echo $BNB_CUDA_VERSION
!echo $LD_LIBRARY_PATH

!pip install transformers
!pip install datasets
!pip install peft
!pip install wandb
!pip install trl

import os
import torch
import json
import copy
import datasets
from datasets import Dataset, load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    Trainer,  # Changed from SFTTrainer to Trainer
    TrainingArguments,
    logging
)
from peft import (
    prepare_model_for_kbit_training,
    LoraConfig,
    get_peft_model,
    PeftModel
)
import wandb
import argparse
from dataclasses import dataclass
from typing import Dict, List, Sequence

from google.colab import drive
drive.mount('/content/drive')

base_path = "/content/drive/MyDrive/Academic_Professional_Planning/SPRING2025/GEN_AI/Multi_Agent_Self_Debugger_Coder/training"
data_path = os.path.join(base_path, "data/train_data.json")
dataset = load_dataset('json', data_files=data_path, split="train", cache_dir="path_to_cache_dir")
output_dir = os.path.join(base_path, "finetuned/deepseek_coder_finetuned")

model_id = "deepseek-ai/deepseek-coder-6.7b-instruct"

# Constants needed for DeepSeek's original approach
IGNORE_INDEX = -100
EOT_TOKEN = "<|end|>"

def parse_args():
    parser = argparse.ArgumentParser(description="Fine-tune DeepSeek Coder with LoRA")
    parser.add_argument("--model_id", type=str, default="deepseek-ai/deepseek-coder-6.7b-instruct",
                        help="Model ID to fine-tune")
    parser.add_argument("--data_path", type=str, default=None,
                        help="Path to JSON dataset file")
    parser.add_argument("--output_dir", type=str, default=output_dir,
                        help="Directory to save model")
    parser.add_argument("--epochs", type=int, default=3,
                        help="Number of training epochs")
    parser.add_argument("--batch_size", type=int, default=8,
                        help="Batch size for training")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=4,
                        help="Number of gradient accumulation steps")
    parser.add_argument("--learning_rate", type=float, default=2e-4,
                        help="Learning rate for training")
    parser.add_argument("--lora_r", type=int, default=64,
                        help="LoRA attention dimension")
    parser.add_argument("--lora_alpha", type=int, default=16,
                        help="LoRA alpha parameter")
    parser.add_argument("--max_seq_length", type=int, default=768,
                        help="Maximum sequence length for training")
    parser.add_argument("--wandb_project", type=str, default="deepseek-finetuning",
                        help="Weights & Biases project name")

    # This allows the script to ignore the unexpected kernel arguments
    args, unknown = parser.parse_known_args()

    return args

def check_gpu():
    """Check GPU availability and print details"""
    print(f"CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA version: {torch.version.cuda}")
        print(f"GPU device: {torch.cuda.get_device_name(0)}")
        print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    print(f"CUDA_HOME: {os.environ.get('CUDA_HOME', 'Not set')}")

def build_instruction_prompt(instruction: str, is_instruct_model: bool):
    """Format instruction according to DeepSeek template"""
    if is_instruct_model:
        return f"<|user|>\n{instruction}\n<|assistant|>\n"
    else:
        return f"Problem:\n{instruction}\n\nSolution:\n"

def _tokenize_fn(strings: Sequence[str], tokenizer: AutoTokenizer) -> Dict:
    """Tokenize a list of strings."""
    tokenized_list = [
        tokenizer(
            text,
            return_tensors="pt",
            padding="longest",
            max_length=tokenizer.model_max_length,
            truncation=True,
        )
        for text in strings
    ]

    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]
    input_ids_lens = labels_lens = [
        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list
    ]

    return dict(
        input_ids=input_ids,
        labels=labels,
        input_ids_lens=input_ids_lens,
        labels_lens=labels_lens,
    )

def preprocess(
    sources: Sequence[str],
    targets: Sequence[str],
    tokenizer: AutoTokenizer,
) -> Dict:
    """Preprocess the data by tokenizing."""
    examples = [s + t for s, t in zip(sources, targets)]
    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]
    input_ids = examples_tokenized["input_ids"]

    labels = copy.deepcopy(input_ids)
    for label, source_len in zip(labels, sources_tokenized["input_ids_lens"]):
        label[:source_len] = IGNORE_INDEX
    return dict(input_ids=input_ids, labels=labels)

@dataclass
class DataCollatorForSupervisedDataset(object):
    """Collate examples for supervised fine-tuning."""
    tokenizer: AutoTokenizer

    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:
        input_ids, labels = tuple([instance[key] for instance in instances] for key in ("input_ids", "labels"))
        input_ids = [torch.tensor(x) for x in input_ids]
        input_ids = torch.nn.utils.rnn.pad_sequence(
            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id
        )
        labels = [torch.tensor(x) for x in labels]
        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)

        return dict(
            input_ids=input_ids,
            labels=labels,
            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),
        )

def setup_model_and_tokenizer(args):
    """Setup quantized model and tokenizer using an alternative approach"""
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(args.model_id, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token

    # Try to use 4-bit quantization with bitsandbytes
    try:
        # Setup quantization configuration
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
        )


        # Load model
        model = AutoModelForCausalLM.from_pretrained(
            args.model_id,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
        )
        print("Successfully loaded model with 4-bit quantization")
    except Exception as e:
        print(f"4-bit quantization failed with error: {e}")
        print("Falling back to 8-bit quantization")

        try:
            # Try 8-bit quantization instead
            model = AutoModelForCausalLM.from_pretrained(
                args.model_id,
                load_in_8bit=True,
                device_map="auto",
                trust_remote_code=True,
            )
            print("Successfully loaded model with 8-bit quantization")
        except Exception as e:
            print(f"8-bit quantization failed with error: {e}")
            print("Falling back to fp16 precision without quantization")

            # Fall back to fp16 without quantization
            model = AutoModelForCausalLM.from_pretrained(
                args.model_id,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
            )
            print("Successfully loaded model with fp16 precision (no quantization)")

    # Prepare model for training
    if hasattr(model, "is_loaded_in_8bit") or hasattr(model, "is_loaded_in_4bit"):
        model = prepare_model_for_kbit_training(model)

    # Define LoRA configuration
    peft_config = LoraConfig(
        lora_alpha=args.lora_alpha,
        lora_dropout=0.1,
        r=args.lora_r,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=[
            "q_proj",
            "k_proj",
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj"
        ]
    )

    # Apply LoRA adapter to model
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    return model, tokenizer, peft_config

def prepare_dataset_for_trainer(dataset, tokenizer, args):
    """Prepare dataset in the format expected by the Trainer with custom collator"""
    is_instruct_model = "instruct" in args.model_id.lower()

    # Prepare sources and targets
    sources = [
        build_instruction_prompt(example["instruction"], is_instruct_model)
        for example in dataset
    ]

    if is_instruct_model:
        targets = [f"{example['output']}\n{EOT_TOKEN}" for example in dataset]
    else:
        targets = [f"{example['output']}" for example in dataset]

    # Process the data using DeepSeek's original approach
    processed_data = preprocess(sources, targets, tokenizer)

    # Convert to Dataset format expected by Trainer
    train_dataset = Dataset.from_dict({
        "input_ids": processed_data["input_ids"],
        "labels": processed_data["labels"]
    })

    return train_dataset

def train_model(model, tokenizer, dataset, args):
    """Train the model using standard HuggingFace Trainer with custom collator"""
    # Initialize wandb if available
    if args.wandb_project:
        try:
            wandb.init(project=args.wandb_project)
        except:
            print("Could not initialize wandb. Continuing without it.")

    # Determine optimizer based on model quantization type
    if hasattr(model, "is_loaded_in_4bit") and model.is_loaded_in_4bit:
        optim_choice = "paged_adamw_8bit"
    elif hasattr(model, "is_loaded_in_8bit") and model.is_loaded_in_8bit:
        optim_choice = "paged_adamw_8bit"
    else:
        optim_choice = "adamw_torch"

    print(f"Using optimizer: {optim_choice}")


    # Training arguments
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        fp16=True,
        logging_steps=50,
        save_strategy="steps",
        save_steps=250,
        save_total_limit=2,
        optim=optim_choice,
        report_to="wandb" if args.wandb_project else "none"
    )

    # Create data collator
    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)

    # Initialize trainer with custom collator
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        data_collator=data_collator,
        tokenizer=tokenizer
    )

    # Start training
    model.train() #added
    trainer.train()

    # Save the fine-tuned model
    trainer.model.save_pretrained(args.output_dir)
    tokenizer.save_pretrained(args.output_dir)

    # Save logs for reproducibility
    trainer.state.save_to_json(os.path.join(args.output_dir, "trainer_state.json"))
    with open(os.path.join(args.output_dir, "trainer_log_history.json"), "w") as f:
        json.dump(trainer.state.log_history, f, indent=2)

    return trainer

def main():
    args = parse_args()
    check_gpu()

    # Setup model and tokenizer
    model, tokenizer, peft_config = setup_model_and_tokenizer(args)

    # Prepare dataset using DeepSeek's original approach
    processed_dataset = prepare_dataset_for_trainer(dataset, tokenizer, args)

    # Train model using standard Trainer with custom collator
    trainer = train_model(model, tokenizer, processed_dataset, args)

    print(f"Fine-tuning complete! Model saved to: {args.output_dir}")

    # Save to Google Drive
    google_drive_path = os.path.join(base_path, "finetuned/deepseek_coder_finetuned")
    print(f"Saving model to Google Drive at {google_drive_path}")
    trainer.model.save_pretrained(google_drive_path)
    tokenizer.save_pretrained(google_drive_path)
    print(f"Model saved to Google Drive at {google_drive_path}")

if __name__ == "__main__":
    main()