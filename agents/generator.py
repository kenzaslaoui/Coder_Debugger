# -*- coding: utf-8 -*-
"""Copy of code_generator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eBHmukVq6EkKqTmYFRjUiCTQygJqkfHx
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

class CodeGenerator:
    def __init__(self, model_name="deepseek-ai/deepseek-coder-6.7b-instruct"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
        ).to(self.device)
        self.model.eval()

    def format_prompt(self, task_description: str, test_cases: list[str]) -> str:
        """
        Combines the task description with formatted assert-based test cases.
        We will now ask the model to format the output with 'code' and 'explanation' sections.
        """
        formatted_tests = "\n".join(test_cases)
        return f"Task:\n{task_description}\n\nExample Test Cases:\n{formatted_tests}\n\n"

    def generate_code(self, task_description: str, test_cases: list[str], max_new_tokens=512) -> str:
        """
        Generates Python code from a natural language task and test cases.
        Returns a tuple: (code, explanation)
        """
        prompt = self.format_prompt(task_description, test_cases)
        messages = [{'role': 'user', 'content': prompt}]
        inputs = self.tokenizer.apply_chat_template(
            messages, add_generation_prompt=True, return_tensors="pt"
        ).to(self.device)

        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                top_k=50,
                top_p=0.95,
                eos_token_id=self.tokenizer.eos_token_id
            )

        # Decode the output and split into 'code' and 'explanation'
        full_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # # Look for "code:" and "explanation:" labels and extract content
        # code_start = full_output.find("code:")
        # explanation_start = full_output.find("explanation:")

        # if code_start == -1 or explanation_start == -1:
        #     raise ValueError("Model output does not match the expected format.")

        # # Extract code and explanation
        # code = full_output[code_start + len("code:"):explanation_start].strip()
        # explanation = full_output[explanation_start + len("explanation:"):].strip()

        return f"Full output:\n{full_output}"