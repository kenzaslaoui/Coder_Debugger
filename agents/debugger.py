# -*- coding: utf-8 -*-
"""Copy of code_debugger.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gFSIZpl99N18WcVXN-Qk-IYI2X1LrGF_
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

class CodeDebugger:
    def __init__(self, model_name="deepseek-ai/deepseek-coder-6.7b-instruct"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16
        ).to(self.device)
        self.model.eval()

    def format_debug_prompt(self, prompt: str, buggy_code: str, error: str) -> str:
        return (
            f"You are a helpful AI coding assistant.\n"
            f"The user was trying to solve the following task:\n"
            f"{prompt}\n\n"
            f"The code they wrote is:\n```python\n{buggy_code}\n```\n"
            f"When they ran the code, they got this error:\n{error}\n\n"
            f"Please provide the corrected version of the code.\n"
        )

    def debug_code(self, prompt: str, buggy_code: str, error: str, max_new_tokens: int = 512) -> str:
        full_prompt = self.format_debug_prompt(prompt, buggy_code, error)
        messages = [{'role': 'user', 'content': full_prompt}]
        inputs = self.tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to(self.device)

        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                top_k=50,
                top_p=0.95,
                eos_token_id=self.tokenizer.eos_token_id
            )

        fixed_code = self.tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)
        return fixed_code.strip()