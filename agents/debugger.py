# -*- coding: utf-8 -*-
"""Finetuned_Debugger.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iaz0ZuZBB0BavXQkrap6lroYva2mYRqJ
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel


# Setup device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Path to the fine-tuned model adapter
ADAPTER_PATH = "/content/drive/MyDrive/Academic_Professional_Planning/SPRING2025/GEN_AI/Multi_Agent_Self_Debugger_Coder/training/finetuned/deepseek_coder_finetuned/final_finetuned_model"
BASE_MODEL = "deepseek-ai/deepseek-coder-6.7b-instruct"

# Create a debugger class that uses the fine-tuned model
class CodeDebugger:
    def __init__(self, model_path=ADAPTER_PATH, base_model=BASE_MODEL):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Load tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)

        # Load base model
        print(f"Loading base model: {base_model}")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )

        # Load adapter
        print(f"Loading fine-tuned adapter from: {model_path}")
        self.model = PeftModel.from_pretrained(base_model, model_path)
        self.model.eval()

        print("Fine-tuned model loaded successfully!")

    def format_debug_prompt(self, prompt: str, buggy_code: str, error: str) -> str:
        return (
            f"Given a programming task and its incorrect solution, your task is to fix up the incorrect solution according to the programming task and provide the correct, executable solution.\n\n"
            f"###Task:\n\n"
            f"{prompt}\n\n"
            f"###Incorrect Solution:\n\n"
            f"{buggy_code}\n"
            f"###Error:\n\n"
            f"{error}\n"
        )


    def debug_code(self, prompt: str, buggy_code: str, error: str, max_new_tokens: int = 512) -> str:

        full_prompt = self.format_debug_prompt(prompt, buggy_code, error)
        messages = [{'role': 'user', 'content': full_prompt}]

        try:
            inputs = self.tokenizer.apply_chat_template(
                messages,
                add_generation_prompt=True,
                return_tensors="pt"
            ).to(self.device)

            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=False,
                    top_k=50,
                    top_p=0.95,
                    temperature=0.2,
                    eos_token_id=self.tokenizer.eos_token_id
                )

            fixed_code = self.tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)
            return fixed_code.strip()
        except Exception as e:
            print(f"Error in generation: {e}")
            return f"Error in generation: {e}"