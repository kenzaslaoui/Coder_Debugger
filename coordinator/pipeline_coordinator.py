# -*- coding: utf-8 -*-
"""coordinator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14__HoaNLXSP_LgLAZFlK_japvexKslrH
"""

"""
If you want to run the pipeline,
you need to mention the programming language in the task string. 
Also, ensure the asserts are formatted correctly given your 
programming language of choice.
"""


!pip install langchain openai
!pip install --upgrade langchain langchain-community openai

import warnings
warnings.filterwarnings("ignore")

import pprint
from google.colab import userdata
import openai
import os
import sys
import importlib.util
from langchain_community.chat_models import ChatOpenAI
from langchain_core.messages import HumanMessage
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
openai.api_key = userdata.get('OPENAI_API_KEY')
os.environ['OPENAI_API_KEY'] = openai.api_key
from google.colab import drive
drive.mount('/content/drive')



class Coordinator:
    def __init__(self):
        base_path = "/content/drive/MyDrive/Academic_Professional_Planning/SPRING2025/GEN_AI/Multi_Agent_Self_Debugger_Coder/agents"

        def load_class(module_name, file_path, class_name):
            spec = importlib.util.spec_from_file_location(module_name, file_path)
            mod = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(mod)
            return getattr(mod, class_name)

        CodeGenerator = load_class("generator", f"{base_path}/generator.py", "CodeGenerator")
        ExecutionManager = load_class("executor", f"{base_path}/executor.py", "CodeExecutor")
        CodeDebugger = load_class("debugger", f"{base_path}/debugger.py", "CodeDebugger")

        self.generator = CodeGenerator()
        self.executor = ExecutionManager()
        self.debugger = CodeDebugger()
        self.parser_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    def parse_code_from_output(self, full_output: str, language) -> str:
        """Uses GPT-4o-mini to extract just the code from the LLM's full response."""
        parse_prompt = (
            f"You are a helpful assistant that extracts only the raw {language} code"
            f"from a mixed natural language and code response. "
            f"Please return only the raw {language} code without any quotes, triple quotes, or other string delimiters. Do not include any extra explanation or markdown. Just the plain code."
            f"Also, please exclude any test cases from the code your return. However, if #include <cassert> is included keep it there"
            f"Most importantly remember only raw code and no quotes"
            f"Input:\n{full_output}"
        )
        response = self.parser_llm([HumanMessage(content=parse_prompt)])
        return response.content.strip()

    def run_pipeline(self, task: str, test_cases: list[str], language: str = "python"):
        """
        Executes the full pipeline for a given task and language.

        Args:
            task: The programming task description
            test_cases: List of test cases
            language: Target language ('python', 'cpp', or 'java')
        """
        # Prompt (Task + test cases)
        prompt = f"Task:\n{task}\nExample Test Cases:\n" + "\n".join(test_cases)

        # Generate full output (task, code, explanation)
        generator_full_output = self.generator.generate_code(task, test_cases)
        print(f"Generator's output:\n{generator_full_output}")

        # Extract just the code with the gpt 4 llm
        generator_code = self.parse_code_from_output(generator_full_output, language)
        if not generator_code.strip():
            raise ValueError("Parsed generator code is empty. Check generator output format.")
        print(f"\n\nParsed generator's code:\n{generator_code}")

        # Execute the generator's code and evaluate
        generator_result = self.executor.run_code(generator_code, test_cases, language=language)
        if generator_result["success"]:
            print(f"\n\nExecution of generator's code successful!")
            final_output = {
                "generator's code": generator_code,
                "success": True,
                "stdout": generator_result["stdout"]
            }
            return pprint.pprint(final_output)

        # Debug if failed
        print(f"\n\nExecution of generator's code failed. Invoking debugger.")
        generator_error_msg = (
                   f"STDERR:\n{generator_result['stderr']}\n\nSTDOUT:\n{generator_result['stdout']}"
        )
        print(f"\n\nGenerator's error message:\n{generator_error_msg}")

        debugger_output = self.debugger.debug_code(prompt, generator_code, generator_error_msg)

        print(f"\n\nDebugger's Generated output:\n{debugger_output}")

        # Extract just the code with the gpt 4 llm
        fixed_code = self.parse_code_from_output(debugger_output, language)
        if not fixed_code.strip():
            raise ValueError("Parsed debugger's code is empty. Check debugger's output format.")
        print(f"\n\nParsed debugger's code:\n{fixed_code}")

        # Execute the debugger's code and evaluate
        debugger_result = self.executor.run_code(fixed_code, test_cases, language=language)
        if debugger_result["success"]:
            print("Execution of debugger's code successful!")
            final_output = {
                "generator's code": generator_code,
                "generator's success": False,
                "debugger's code": fixed_code,
                "debugger's success": True,
                "stdout": debugger_result["stdout"]
            }
            return pprint.pprint(final_output)

        print(f"\n\nExecution of debugger code failed. Try again later!")
        debug_error_msg = (
            f"STDERR:\n{generator_result['stderr']}\n\nSTDOUT:\n{generator_result['stdout']}"
        )
        final_output = {
            "generator's code": generator_code,
            "generator's success": False,
            "debugger's code": fixed_code,
            "debugger's success": False,
            "error": debug_error_msg
        }
        return pprint.pprint(final_output)